{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2.0 Shakespeare Text CharRNN.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM7oDB6b2pKAAqQyPZlngzv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"6xj6rNhCo2Js"},"source":["## **Project: Generating Shakespearean Text Using a Character RNN**\n","\n","In a famous 2015 blog post titled “The Unreasonable Effectiveness of Recurrent Neural\n","Networks,” Andrej Karpathy showed how to train an RNN to predict the next\n","character in a sentence. This **Char-RNN** can then be used to generate novel text, one\n","character at a time. Here is a small sample of the text generated by a Char-RNN\n","model after it was trained on all of Shakespeare’s work:\n","PANDARUS:\n","Alas, I think he shall be come approached and the day\n","when little rain would be attain’d into being never fed,\n","and who is but a chain and subjects of his death,\n","I should not sleep."]},{"cell_type":"markdown","metadata":{"id":"CE6-lw5Ukdwc"},"source":["## **Natural Language Processing Processing (NLP)**\n","A common approach of **NLP** is to use **RNN**. a *character* RNN, trained to predict the next character in a sentence. This will allow us to generate some original text, and in the process we will see how to build a Tensorflow dataset on a very long sequence.\n","\n","\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"umREWl6qzNKy"},"source":["### **Building a Char-RNN**\n","- **(Char-RNN)** character RNN, *trained to predict the next character in a sentence*\n","- *Trained to generate a text, one characater at a time.*\n"]},{"cell_type":"markdown","metadata":{"id":"2L0GHWvz0eJ4"},"source":["### **Loading data and preparing dataset**\n","- import libraries\n","- load dataset with filepath\n","- run the  first 100 characters "]},{"cell_type":"code","metadata":{"id":"VezF8IR8sVpQ"},"source":["# import libraries\n","import os\n","import sys\n","import numpy as np\n","import sklearn\n","import tensorflow as tf\n","from tensorflow import keras\n","import numpy as np\n","\n","# plot figures\n","%matplotlib inline\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uWpWUqDJohmm"},"source":["# load dataset with filepath\n","shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hVo-tLPT52VM","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1603389758504,"user_tz":240,"elapsed":565,"user":{"displayName":"Tenley Wiltshire","photoUrl":"https://lh4.googleusercontent.com/-_nPyKZbERyU/AAAAAAAAAAI/AAAAAAAAAfM/f_yjD2N5P_0/s64/photo.jpg","userId":"07699677317221241624"}},"outputId":"95d5edeb-72cd-4cdc-8b6f-767a5878e00a"},"source":["filepath = keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n","with open(filepath) as f:\n","    shakespeare_text = f.read()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading data from https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n","1122304/1115394 [==============================] - 0s 0us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GZsO4AHL65bN","colab":{"base_uri":"https://localhost:8080/","height":167},"executionInfo":{"status":"ok","timestamp":1603389777271,"user_tz":240,"elapsed":422,"user":{"displayName":"Tenley Wiltshire","photoUrl":"https://lh4.googleusercontent.com/-_nPyKZbERyU/AAAAAAAAAAI/AAAAAAAAAfM/f_yjD2N5P_0/s64/photo.jpg","userId":"07699677317221241624"}},"outputId":"daa8ce74-e00c-4742-ad03-374310631ddd"},"source":["# first 100 characters\n","print(shakespeare_text[:100])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["First Citizen:\n","Before we proceed any further, hear me speak.\n","\n","All:\n","Speak, speak.\n","\n","First Citizen:\n","You\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QfYXxM_t0_KI"},"source":["Next we encode every characater as an integer using a Keras's Tokenizer class. First we need to **fit a tokenizer to the text**: it will find all the characters used in the text and map each of them to different character ID, **from 1 to the number of distinct character ID** (does not start from 0)\n","- **fit_on_texts method** encodes words/texts: tokenize a list of sentence (**character ID's from 1 to ->**)\n","\n","#### **Encode every character as an integer**\n","- get keras tokenizer class with character-level encoding\n","- encode character in text method"]},{"cell_type":"code","metadata":{"id":"iWkaEvpu_pZw"},"source":["# keras tokenizer class\n","tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n","# char in text method\n","tokenizer.fit_on_texts(shakespeare_text)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bZM6NXBUCmZI"},"source":["**Tokenize sequence of the word \"First\"**\n","1. text to sequences\n","2. sequences to text\n","3. show number of distinct 's ID\n","4.  show the total number of characters (dataset_size of document_count)\n","5. Let’s encode the full text so each character is represented by its ID (we subtract 1 to\n","get IDs from 0 to 38, rather than from 1 to 39)"]},{"cell_type":"code","metadata":{"id":"OG8wDUxrDI0K","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1603389787600,"user_tz":240,"elapsed":662,"user":{"displayName":"Tenley Wiltshire","photoUrl":"https://lh4.googleusercontent.com/-_nPyKZbERyU/AAAAAAAAAAI/AAAAAAAAAfM/f_yjD2N5P_0/s64/photo.jpg","userId":"07699677317221241624"}},"outputId":"9abe360c-aca6-4a4e-fffd-d6b1dc972f1d"},"source":["# text to sequences\n","tokenizer.texts_to_sequences([\"First\"])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[20, 6, 9, 8, 3]]"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"cPMZMz2mENYg","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1603389790782,"user_tz":240,"elapsed":418,"user":{"displayName":"Tenley Wiltshire","photoUrl":"https://lh4.googleusercontent.com/-_nPyKZbERyU/AAAAAAAAAAI/AAAAAAAAAfM/f_yjD2N5P_0/s64/photo.jpg","userId":"07699677317221241624"}},"outputId":"8257cf46-1bef-461c-c8fa-2bf488b9f784"},"source":["# sequences to text\n","tokenizer.sequences_to_texts([[20,6,9,8,3]])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['f i r s t']"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"-ASonYM0Eofg","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1603389800997,"user_tz":240,"elapsed":285,"user":{"displayName":"Tenley Wiltshire","photoUrl":"https://lh4.googleusercontent.com/-_nPyKZbERyU/AAAAAAAAAAI/AAAAAAAAAfM/f_yjD2N5P_0/s64/photo.jpg","userId":"07699677317221241624"}},"outputId":"6a599e69-24e3-4df3-f33d-4273183594b6"},"source":["# number of distinct chars ID\n","max_id = len(tokenizer.word_index)\n","print(max_id)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["39\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"G4ms1dnpFZBd","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1603389802950,"user_tz":240,"elapsed":395,"user":{"displayName":"Tenley Wiltshire","photoUrl":"https://lh4.googleusercontent.com/-_nPyKZbERyU/AAAAAAAAAAI/AAAAAAAAAfM/f_yjD2N5P_0/s64/photo.jpg","userId":"07699677317221241624"}},"outputId":"812fb856-6b38-4a5f-eeb4-3bb11db7be81"},"source":["# dataset_size (total number of characters)\n","dataset_size = tokenizer.document_count\n","print(dataset_size)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1115394\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YkuxpaClGKd4","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1603389805230,"user_tz":240,"elapsed":820,"user":{"displayName":"Tenley Wiltshire","photoUrl":"https://lh4.googleusercontent.com/-_nPyKZbERyU/AAAAAAAAAAI/AAAAAAAAAfM/f_yjD2N5P_0/s64/photo.jpg","userId":"07699677317221241624"}},"outputId":"f45cffcd-5953-4504-c9da-d4cc1b81caf2"},"source":["# encode full text\n","[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1\n","print([encoded])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[array([19,  5,  8, ..., 20, 26, 10])]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BoioP0F0Ij7l"},"source":["### **Split Dataset into Training set and Validation set**\n","Now back to Shakespeare! Let’s take the first 90% of the text for the training set\n","(keeping the rest for the validation set and the test set), and create a tf.data.Dataset\n","that will return each character one by one from this set:\n","- Train text size to 90%\n"]},{"cell_type":"code","metadata":{"id":"VXKYDo5kSGFm"},"source":["# train text size to 90%\n","train_size = dataset_size * 90// 100\n","dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VdzIr-QYWNUd"},"source":["### **Split Dataset into Multiple windows**\n","Use the **dataset's window() method**. long sequence of characters into smaller windows of text. **Truncated backprpagation through time**. Every instance in the dataset\n","will be a fairly short substring of the whole text, and the RNN will be unrolled\n","only over the length of these substrings.\n","- Call the window() method to create a dataset of short text windows\n","- Use shift=1 to get characters 1 to 101\n","- Use drop_remainder=True to ensure all windows are 101 characters long(this allow us to create batches without padding)\n","- Shuffle and batch the windows separate the inputs\n","- Separate the input(the first 100 chars) from the target(last characs)"]},{"cell_type":"code","metadata":{"id":"s0oIZNQGIrKr"},"source":["# create dataset fo short text windows\n","n_steps =100\n","window_length = n_steps + 1 # target input shifted 1 character ahead\n","dataset = dataset.window(window_length, shift=1, drop_remainder=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RnnjkW24ZZIv"},"source":["**The window()** method: creates a dataset that contains windows, each of which is also represented as a **nested dataset.** we cannot use a nested dataset directly for\n","training, as our model will expect **tensors(vectors an matrices inputs)** as input, not datasets. \n","\n","We call the **flat_map()** method it convert a nested dataset into a flat dataset(not containing datasets). It takes\n","a function as an argument, which allows you to transform each dataset in the nested\n","dataset before flattening. For example, if you pass the function lambda ds:\n","ds.batch(2) to flat_map(), then it will transform the nested dataset {{1, 2}, {3, 4, 5,\n","6}} into the flat dataset {[1, 2], [3, 4], [5, 6]}: it’s a dataset of tensors of size 2. With that\n","in mind, we are ready to flatten our dataset:\n","- pass the lambda ds into the **flat method()**\n","- call the **batch(window_length) method** on each window (all windows have same length)"]},{"cell_type":"code","metadata":{"id":"LfizNsaublK_"},"source":["dataset = dataset.flat_map(lambda window: window.batch(window_length))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5HgzQyaWf5lD"},"source":["### **Linear Regression Review**\n","**Linear Regression**: is a supervised machine learning algorithm where the predicted output is continuous and has a constant slope. It’s used to predict values within a continuous range, (e.g. sales, price) rather than trying to classify them into categories (e.g. cat, dog)\n","\n","**Cost function:** The cost function is calculated as an average of loss functions.\n","\n","**loss function: (how well the network is doing)** is a value which is calculated at every instance. It is a part of the cost function\n","**Loss:** Neural networks trained using an optimization process that requires a loss function to calculate the model error\n","\n","    loss = 'categorical_crossentropy' ( ouput probability/how well the network is doing) targets one-hot-coded.\n","    loss = sparse_categorical_crossentropy targets integer and binary_crossentrophy targets two images\n","\n"]},{"cell_type":"markdown","metadata":{"id":"F1VrM_7PkfdV"},"source":["#### **Shuffle and batch windows**\n","- make output stable\n","- batch size = 32\n","- shuffle 10,000 batch sizes\n","- encode each character using a one-hot vector \n","- ad **prefectching:** a mechanism used to pull information out of memory in advance of its use.\n","- print batch shape and size\n","\n"]},{"cell_type":"code","metadata":{"id":"iX6CROZfd2Xx"},"source":["# output stable across runs\n","np.random.seed(42)\n","tf.random.set_seed(42)# output stable across runs\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g5lb1eMal4o8"},"source":["# batch and shuffle windows\n","batch_size = 32\n","dataset = dataset.shuffle(1000).batch(batch_size)\n","dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x_5Nz8YZmNsW"},"source":["# encode each character\n","dataset = dataset.map(\n","    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7RlAc8w4nVnf"},"source":["# add prefectching (pull infor from memory)\n","dataset = dataset.prefetch(1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qVsfvdFKn2XE","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1603390065086,"user_tz":240,"elapsed":521,"user":{"displayName":"Tenley Wiltshire","photoUrl":"https://lh4.googleusercontent.com/-_nPyKZbERyU/AAAAAAAAAAI/AAAAAAAAAfM/f_yjD2N5P_0/s64/photo.jpg","userId":"07699677317221241624"}},"outputId":"731783d2-5de6-4c33-d7d7-340aa31d50c4"},"source":["# batch shape\n","for X_batch, Y_batch in dataset.take(1):\n","  print(X_batch.shape, Y_batch.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(32, 100, 39) (32, 100)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5sCO3QIMteCY"},"source":["### **Build(layers) & Compile neural network with history**\n","    - **Sequential**: SEQUENCE of layers in the neural network.\n","    - **Input Shape**: the first convolution.\n","    - **Dropouts**: They remove a random layer of neurons in your network, thus spedding up network\n","    - **drop_rate** = 0.5: Dropout after each laer\n","    - **Dense layer**: feeds all outputs from the previous layer to all its neurons, each neuron providing one output to the next layer\n","\n","    - **GRU** Gated Recurrent Unit: Rest an update gate\n","    - **LSTM** Long short Term Memory in RNN architecture. The input and output gate\n","    - **TimeDistributed** (add a wrapper) Dense applies a same Dense (fully-connected) operation to every timestep of a 3D tensor.\n","\n","\n","**Compile**\n","- **Loss** (*output probability*) how good the predictions are. - **Optimizer**,(*optimal values*) generates new predictions\n","- **loss** = **sparse_categorical_crossentropy** targets integer "]},{"cell_type":"markdown","metadata":{"id":"-nRWLwZutpV8"},"source":["## **Creating and Training the Model**\n","   -  2 layers keras layer GRU 128 with return seqences, recurrent dropout 0.2\n","    - TimeDistribution: Dense layer with max_id, activation softmax\n","    - loss sparse_categorical crossentropy\n","    - optimizer adam, epochs 5\n","    \n","    "]},{"cell_type":"code","metadata":{"id":"UrqTRed3tbO0","colab":{"base_uri":"https://localhost:8080/","height":74},"executionInfo":{"status":"ok","timestamp":1603390076859,"user_tz":240,"elapsed":1055,"user":{"displayName":"Tenley Wiltshire","photoUrl":"https://lh4.googleusercontent.com/-_nPyKZbERyU/AAAAAAAAAAI/AAAAAAAAAfM/f_yjD2N5P_0/s64/photo.jpg","userId":"07699677317221241624"}},"outputId":"8d502fd4-3491-4709-98e9-1eb1c5a20efa"},"source":["model = keras.models.Sequential([\n","        keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],\n","        dropout=0.2, recurrent_dropout=0.2),\n","        keras.layers.GRU(128, return_sequences=True,\n","        dropout=0.2, recurrent_dropout=0.2),\n","        keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n","        activation=\"softmax\"))\n","])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Layer gru will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n","WARNING:tensorflow:Layer gru_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"v4B-gArhFR0F"},"source":["model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n","history = model.fit(dataset, steps_per_epoch=train_size // batch_size,\n","                    epochs=5)"]},{"cell_type":"markdown","metadata":{"id":"02asL4_OFzLr"},"source":["### **Using Model to Generate Text**\n","Now we have a model that can predict the next character in text written by Shakespeare.\n","To feed it some text, we first need to preprocess it like we did earlier, so let’s\n","create a little function for this:\n","- create preprocess texts function\n","- predict the next letter in some text\n"]},{"cell_type":"code","metadata":{"id":"ZznZimBEFRxh"},"source":["# preprocess texts\n","def preprocess(texts):\n","  X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n","  return tf.one_hot(X, max_id)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yx4cyoU3GmvU","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1603395360640,"user_tz":240,"elapsed":274,"user":{"displayName":"Tenley Wiltshire","photoUrl":"https://lh4.googleusercontent.com/-_nPyKZbERyU/AAAAAAAAAAI/AAAAAAAAAfM/f_yjD2N5P_0/s64/photo.jpg","userId":"07699677317221241624"}},"outputId":"e92dd1cf-0ea0-42ce-f087-072cbf7d07b6"},"source":["# predict next letter\n","X_new = preprocess([\"How are yo\"])\n","Y_pred = model.predict_classes(X_new)\n","tokenizer.sequences_to_texts(Y_pred + 1)[0][-1] # 1st sentence, last char"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'u'"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"markdown","metadata":{"id":"B3kG3St1ITnv"},"source":["We can\n","pick the next character randomly, with a probability equal to the estimated probability,\n","using TensorFlow’s tf.random.categorical() function. This will generate more\n","diverse and interesting text. The categorical() function samples random class indices,\n","given the class log probabilities (logits). To have more control over the diversity\n","of the generated text, we can divide the logits by a number called the temperature,\n","which we can tweak as we wish: a temperature close to 0 will favor the highprobability\n","characters, while a very high temperature will give all characters an equal\n","probability. The following next_char() function uses this approach to pick the next\n","character to add to the input text:\n","- use the next char() function to pick the next character to add to the input text\n","- pick next character randomly using tf random categorical() function\n","- write a function that will repeatedly call the next_char()to get the next character and append it to the given text"]},{"cell_type":"code","metadata":{"id":"KdAQXUuAKXFQ"},"source":["# function to pick next character\n","def next_char(text, temperature=1):\n","  X_new = preprocess([text])\n","  y_proba = model.predict(X_new)[0, -1:, :]\n","  rescaled_logits = tf.math.log(y_proba) / temperature\n","  char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n","  return tokenizer.sequences_to_texts(char_id.numpy())[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Um_o7kf-Kzyo"},"source":["# function to call the nex_char() function\n","def complete_text(text, n_chars=50, temperature=1):\n","  for _ in range(n_chars):\n","    text += next_char(text, temperature)\n","  return text"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AnnHUlb1RwEI"},"source":["**We are now ready to generate some test**\n","- print complete_text 't' temperature=0.2\n","- print complete_text('w' temperature=1\n","- prnt complete_text 'w' temperature=2"]},{"cell_type":"code","metadata":{"id":"XrP4ZA-KLpkh","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1603395373041,"user_tz":240,"elapsed":3255,"user":{"displayName":"Tenley Wiltshire","photoUrl":"https://lh4.googleusercontent.com/-_nPyKZbERyU/AAAAAAAAAAI/AAAAAAAAAfM/f_yjD2N5P_0/s64/photo.jpg","userId":"07699677317221241624"}},"outputId":"d36788fa-f261-4a9b-842d-cce3ae90b477"},"source":["print(complete_text(\"t\", temperature=0.2))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["the death,\n","and you mayor me with the consure of the\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"g1O7QQnjLynw","colab":{"base_uri":"https://localhost:8080/","height":92},"executionInfo":{"status":"ok","timestamp":1603395381123,"user_tz":240,"elapsed":2600,"user":{"displayName":"Tenley Wiltshire","photoUrl":"https://lh4.googleusercontent.com/-_nPyKZbERyU/AAAAAAAAAAI/AAAAAAAAAfM/f_yjD2N5P_0/s64/photo.jpg","userId":"07699677317221241624"}},"outputId":"2df9e20a-4f24-4968-a4e4-ca2565945097"},"source":["print(complete_text(\"w\", temperature=1))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["ward seart.\n","\n","rovell\n","of traitorous his drazentpent m\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"efYgRnUNSXh3","colab":{"base_uri":"https://localhost:8080/","height":92},"executionInfo":{"status":"ok","timestamp":1603395390136,"user_tz":240,"elapsed":2353,"user":{"displayName":"Tenley Wiltshire","photoUrl":"https://lh4.googleusercontent.com/-_nPyKZbERyU/AAAAAAAAAAI/AAAAAAAAAfM/f_yjD2N5P_0/s64/photo.jpg","userId":"07699677317221241624"}},"outputId":"d664d6e3-81f3-41cd-8748-ef0affc79763"},"source":["print(complete_text(\"w\", temperature=2))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["wel hase?\n","\n","?gloucester:\n","valg deain rulk-your hisnda\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SrKjrrdASnu0"},"source":["#### **Summary**\n","From the results above, it appears that our texts works best with temperatures close to 1.To generate\n","more convincing text, you could try using more GRU layers and more neurons per\n","layer, train for longer, and add some regularization (for example, you could set recur\n","rent_dropout=0.3 in the GRU layers). Moreover, the model is currently incapable of\n","learning patterns longer than n_steps, which is just 100 characters."]}]}